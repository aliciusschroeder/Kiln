From 2bb3393dc42557d083f095b65a70d5a079d985cc Mon Sep 17 00:00:00 2001
From: "Leonard Q. Marcq" <marcqleonard@gmail.com>
Date: Wed, 14 May 2025 01:24:39 +0800
Subject: [PATCH 1/8] refactor: extract job runner into own util

---
 .../core/kiln_ai/adapters/eval/eval_runner.py | 69 ++-------------
 libs/core/kiln_ai/utils/async_job_runner.py   | 86 +++++++++++++++++++
 .../kiln_ai/utils/test_async_job_runner.py    | 36 ++++++++
 3 files changed, 127 insertions(+), 64 deletions(-)
 create mode 100644 libs/core/kiln_ai/utils/async_job_runner.py
 create mode 100644 libs/core/kiln_ai/utils/test_async_job_runner.py

diff --git a/libs/core/kiln_ai/adapters/eval/eval_runner.py b/libs/core/kiln_ai/adapters/eval/eval_runner.py
index c8a30753..50c10193 100644
--- a/libs/core/kiln_ai/adapters/eval/eval_runner.py
+++ b/libs/core/kiln_ai/adapters/eval/eval_runner.py
@@ -1,4 +1,3 @@
-import asyncio
 import logging
 from dataclasses import dataclass
 from typing import AsyncGenerator, Dict, List, Literal, Set
@@ -10,6 +9,7 @@
 from kiln_ai.datamodel.eval import EvalConfig, EvalRun, EvalScores
 from kiln_ai.datamodel.task import TaskRunConfig
 from kiln_ai.datamodel.task_run import TaskRun
+from kiln_ai.utils.async_job_runner import AsyncJobRunner, Progress
 
 logger = logging.getLogger(__name__)
 
@@ -23,13 +23,6 @@ class EvalJob:
     task_run_config: TaskRunConfig | None = None
 
 
-@dataclass
-class EvalProgress:
-    complete: int | None = None
-    total: int | None = None
-    errors: int | None = None
-
-
 class EvalRunner:
     """
     Runs an eval. Async execution is supported to make it faster when using remote/fast model providers.
@@ -161,67 +154,15 @@ def collect_tasks_for_task_run_eval(self) -> List[EvalJob]:
             if task_run.id not in already_run[eval_config.id][run_config.id]
         ]
 
-    async def run(self, concurrency: int = 25) -> AsyncGenerator[EvalProgress, None]:
+    async def run(self, concurrency: int = 25) -> AsyncGenerator[Progress, None]:
         """
         Runs the configured eval run with parallel workers and yields progress updates.
         """
         jobs = self.collect_tasks()
 
-        complete = 0
-        errors = 0
-        total = len(jobs)
-
-        # Send initial status
-        yield EvalProgress(complete=complete, total=total, errors=errors)
-
-        worker_queue: asyncio.Queue[EvalJob] = asyncio.Queue()
-        for job in jobs:
-            worker_queue.put_nowait(job)
-
-        # simple status queue to return progress. True=success, False=error
-        status_queue: asyncio.Queue[bool] = asyncio.Queue()
-
-        workers = []
-        for i in range(concurrency):
-            task = asyncio.create_task(self.run_worker(worker_queue, status_queue))
-            workers.append(task)
-
-        # Send status updates until workers are done, and they are all sent
-        while not status_queue.empty() or not all(worker.done() for worker in workers):
-            try:
-                # Use timeout to prevent hanging if all workers complete
-                # between our while condition check and get()
-                success = await asyncio.wait_for(status_queue.get(), timeout=0.1)
-                if success:
-                    complete += 1
-                else:
-                    errors += 1
-
-                yield EvalProgress(complete=complete, total=total, errors=errors)
-            except asyncio.TimeoutError:
-                # Timeout is expected, just continue to recheck worker status
-                # Don't love this but beats sentinels for reliability
-                continue
-
-        # These are redundant, but keeping them will catch async errors
-        await asyncio.gather(*workers)
-        await worker_queue.join()
-
-    async def run_worker(
-        self, worker_queue: asyncio.Queue[EvalJob], status_queue: asyncio.Queue[bool]
-    ):
-        while True:
-            try:
-                job = worker_queue.get_nowait()
-            except asyncio.QueueEmpty:
-                # worker can end when the queue is empty
-                break
-            try:
-                success = await self.run_job(job)
-                await status_queue.put(success)
-            finally:
-                # Always mark the dequeued task as done, even on exceptions
-                worker_queue.task_done()
+        runner = AsyncJobRunner(concurrency=concurrency)
+        async for progress in runner.run(jobs, self.run_job):
+            yield progress
 
     async def run_job(self, job: EvalJob) -> bool:
         try:
diff --git a/libs/core/kiln_ai/utils/async_job_runner.py b/libs/core/kiln_ai/utils/async_job_runner.py
new file mode 100644
index 00000000..546b2e8e
--- /dev/null
+++ b/libs/core/kiln_ai/utils/async_job_runner.py
@@ -0,0 +1,86 @@
+import asyncio
+from dataclasses import dataclass
+from typing import AsyncGenerator, Awaitable, Callable, List, TypeVar
+
+T = TypeVar("T")
+
+
+@dataclass
+class Progress:
+    complete: int | None = None
+    total: int | None = None
+    errors: int | None = None
+
+
+class AsyncJobRunner:
+    def __init__(self, concurrency: int = 1):
+        self.concurrency = concurrency
+
+    async def run(
+        self,
+        jobs: List[T],
+        run_job: Callable[[T], Awaitable[bool]],
+    ) -> AsyncGenerator[Progress, None]:
+        """
+        Runs the jobs with parallel workers and yields progress updates.
+        """
+        complete = 0
+        errors = 0
+        total = len(jobs)
+
+        # Send initial status
+        yield Progress(complete=complete, total=total, errors=errors)
+
+        worker_queue: asyncio.Queue[T] = asyncio.Queue()
+        for job in jobs:
+            worker_queue.put_nowait(job)
+
+        # simple status queue to return progress. True=success, False=error
+        status_queue: asyncio.Queue[bool] = asyncio.Queue()
+
+        workers = []
+        for _ in range(self.concurrency):
+            task = asyncio.create_task(
+                self._run_worker(worker_queue, status_queue, run_job)
+            )
+            workers.append(task)
+
+        # Send status updates until workers are done, and they are all sent
+        while not status_queue.empty() or not all(worker.done() for worker in workers):
+            try:
+                # Use timeout to prevent hanging if all workers complete
+                # between our while condition check and get()
+                success = await asyncio.wait_for(status_queue.get(), timeout=0.1)
+                if success:
+                    complete += 1
+                else:
+                    errors += 1
+
+                yield Progress(complete=complete, total=total, errors=errors)
+            except asyncio.TimeoutError:
+                # Timeout is expected, just continue to recheck worker status
+                # Don't love this but beats sentinels for reliability
+                continue
+
+        # These are redundant, but keeping them will catch async errors
+        await asyncio.gather(*workers)
+        await worker_queue.join()
+
+    async def _run_worker(
+        self,
+        worker_queue: asyncio.Queue[T],
+        status_queue: asyncio.Queue[bool],
+        run_job: Callable[[T], Awaitable[bool]],
+    ):
+        while True:
+            try:
+                job = worker_queue.get_nowait()
+            except asyncio.QueueEmpty:
+                # worker can end when the queue is empty
+                break
+            try:
+                success = await run_job(job)
+                await status_queue.put(success)
+            finally:
+                # Always mark the dequeued task as done, even on exceptions
+                worker_queue.task_done()
diff --git a/libs/core/kiln_ai/utils/test_async_job_runner.py b/libs/core/kiln_ai/utils/test_async_job_runner.py
new file mode 100644
index 00000000..01bc8631
--- /dev/null
+++ b/libs/core/kiln_ai/utils/test_async_job_runner.py
@@ -0,0 +1,36 @@
+from unittest.mock import AsyncMock
+
+import pytest
+
+from kiln_ai.utils.async_job_runner import AsyncJobRunner
+
+
+# Test with and without concurrency
+@pytest.mark.parametrize("concurrency", [1, 25])
+@pytest.mark.asyncio
+async def test_async_job_runner_status_updates(concurrency):
+    job_count = 50
+    jobs = [{"id": i} for i in range(job_count)]
+
+    runner = AsyncJobRunner(concurrency=concurrency)
+
+    # fake run_job that succeeds
+    mock_run_job_success = AsyncMock(return_value=True)
+
+    # Expect the status updates in order, and 1 for each job
+    expected_completed_count = 0
+    async for progress in runner.run(jobs, mock_run_job_success):
+        assert progress.complete == expected_completed_count
+        expected_completed_count += 1
+        assert progress.errors == 0
+        assert progress.total == job_count
+
+    # Verify last status update was complete
+    assert expected_completed_count == job_count + 1
+
+    # Verify run_job was called for each job
+    assert mock_run_job_success.call_count == job_count
+
+    # Verify run_job was called with the correct arguments
+    for i in range(job_count):
+        mock_run_job_success.assert_any_await(jobs[i])

From 3116c3fa5d7e0e6a976551886664161f36e24fa6 Mon Sep 17 00:00:00 2001
From: "Leonard Q. Marcq" <marcqleonard@gmail.com>
Date: Wed, 14 May 2025 02:49:02 +0800
Subject: [PATCH 2/8] test: add negative and partial failure cases for async
 job runner

---
 .../kiln_ai/utils/test_async_job_runner.py    | 98 +++++++++++++++++++
 1 file changed, 98 insertions(+)

diff --git a/libs/core/kiln_ai/utils/test_async_job_runner.py b/libs/core/kiln_ai/utils/test_async_job_runner.py
index 01bc8631..83954a08 100644
--- a/libs/core/kiln_ai/utils/test_async_job_runner.py
+++ b/libs/core/kiln_ai/utils/test_async_job_runner.py
@@ -34,3 +34,101 @@ async def test_async_job_runner_status_updates(concurrency):
     # Verify run_job was called with the correct arguments
     for i in range(job_count):
         mock_run_job_success.assert_any_await(jobs[i])
+
+
+@pytest.mark.parametrize("concurrency", [1, 25])
+@pytest.mark.asyncio
+async def test_async_job_runner_all_failures(concurrency):
+    job_count = 50
+    jobs = [{"id": i} for i in range(job_count)]
+
+    runner = AsyncJobRunner(concurrency=concurrency)
+
+    # fake run_job that fails
+    mock_run_job_failure = AsyncMock(return_value=False)
+
+    # Expect the status updates in order, and 1 for each job
+    expected_error_count = 0
+    async for progress in runner.run(jobs, mock_run_job_failure):
+        assert progress.complete == 0
+        assert progress.errors == expected_error_count
+        expected_error_count += 1
+        assert progress.total == job_count
+
+    # Verify last status update was complete
+    assert expected_error_count == job_count + 1
+
+    # Verify run_job was called for each job
+    assert mock_run_job_failure.call_count == job_count
+
+    # Verify run_job was called with the correct arguments
+    for i in range(job_count):
+        mock_run_job_failure.assert_any_await(jobs[i])
+
+
+@pytest.mark.parametrize("concurrency", [1, 25])
+@pytest.mark.asyncio
+async def test_async_job_runner_partial_failures(concurrency):
+    job_count = 50
+    jobs = [{"id": i} for i in range(job_count)]
+
+    # we want to fail on some jobs and succeed on others
+    jobs_to_fail = (0, 2, 4, 6, 8, 20, 25)
+
+    runner = AsyncJobRunner(concurrency=concurrency)
+
+    # fake run_job that fails
+    mock_run_job_partial_success = AsyncMock(
+        # return True for jobs that should succeed
+        side_effect=lambda job: job["id"] not in jobs_to_fail
+    )
+
+    # Expect the status updates in order, and 1 for each job
+    async for progress in runner.run(jobs, mock_run_job_partial_success):
+        assert progress.total == job_count
+
+    # Verify last status update was complete
+    expected_error_count = len([job for job in jobs if job["id"] in jobs_to_fail])
+    expected_success_count = len(jobs) - expected_error_count
+    assert progress.errors == expected_error_count
+    assert progress.complete == expected_success_count
+
+    # Verify run_job was called for each job
+    assert mock_run_job_partial_success.call_count == job_count
+
+    # Verify run_job was called with the correct arguments
+    for i in range(job_count):
+        mock_run_job_partial_success.assert_any_await(jobs[i])
+
+
+@pytest.mark.asyncio
+async def test_async_job_runner_partial_raises():
+    job_count = 50
+    jobs = [{"id": i} for i in range(job_count)]
+
+    # we use concurrency=1 to avoid having the other workers complete jobs
+    # concurrently as that would make it hard to verify when the runner exits
+    runner = AsyncJobRunner(concurrency=1)
+
+    id_to_fail = 10
+
+    def failure_fn(job):
+        if job["id"] == id_to_fail:
+            raise Exception("job failed unexpectedly")
+        return True
+
+    # fake run_job that fails
+    mock_run_job_partial_success = AsyncMock(side_effect=failure_fn)
+
+    # Expect the status updates in order, and 1 for each job
+    # until we hit the job that raises an exception
+    with pytest.raises(Exception, match="job failed unexpectedly"):
+        expected_complete = 0
+        async for progress in runner.run(jobs, mock_run_job_partial_success):
+            assert progress.complete == expected_complete
+            assert progress.errors == 0
+            assert progress.total == job_count
+            expected_complete += 1
+
+    # verify that we yielded progress for jobs all the way up to the job that raised an exception
+    assert expected_complete == id_to_fail + 1

From 5cf9d9d6f586def639f1484b66f0f90c3b1b50df Mon Sep 17 00:00:00 2001
From: "Leonard Q. Marcq" <marcqleonard@gmail.com>
Date: Wed, 14 May 2025 02:56:54 +0800
Subject: [PATCH 3/8] refactor: validate concurrency

Co-authored-by: coderabbitai[bot] <136622811+coderabbitai[bot]@users.noreply.github.com>
---
 libs/core/kiln_ai/utils/async_job_runner.py | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/libs/core/kiln_ai/utils/async_job_runner.py b/libs/core/kiln_ai/utils/async_job_runner.py
index 546b2e8e..a2a856c3 100644
--- a/libs/core/kiln_ai/utils/async_job_runner.py
+++ b/libs/core/kiln_ai/utils/async_job_runner.py
@@ -14,8 +14,9 @@ class Progress:
 
 class AsyncJobRunner:
     def __init__(self, concurrency: int = 1):
+        if concurrency < 1:
+            raise ValueError("concurrency must be ≥ 1")
         self.concurrency = concurrency
-
     async def run(
         self,
         jobs: List[T],

From 633a3a3689a2c416c9637ba574c23229b9cb70f0 Mon Sep 17 00:00:00 2001
From: "Leonard Q. Marcq" <marcqleonard@gmail.com>
Date: Wed, 14 May 2025 03:04:52 +0800
Subject: [PATCH 4/8] chore: format/lint coderabbitai's commit

---
 libs/core/kiln_ai/utils/async_job_runner.py | 1 +
 1 file changed, 1 insertion(+)

diff --git a/libs/core/kiln_ai/utils/async_job_runner.py b/libs/core/kiln_ai/utils/async_job_runner.py
index a2a856c3..38203b2f 100644
--- a/libs/core/kiln_ai/utils/async_job_runner.py
+++ b/libs/core/kiln_ai/utils/async_job_runner.py
@@ -17,6 +17,7 @@ def __init__(self, concurrency: int = 1):
         if concurrency < 1:
             raise ValueError("concurrency must be ≥ 1")
         self.concurrency = concurrency
+
     async def run(
         self,
         jobs: List[T],

From ff3328b388c9b958b6f8273d4cc1adc75ce4171b Mon Sep 17 00:00:00 2001
From: "Leonard Q. Marcq" <marcqleonard@gmail.com>
Date: Wed, 14 May 2025 19:12:20 +0800
Subject: [PATCH 5/8] fix: tests, queue event on fail and wrap unexpected
 errors

---
 libs/core/kiln_ai/utils/async_job_runner.py   | 19 +++-
 .../kiln_ai/utils/test_async_job_runner.py    | 88 ++++++++++++++-----
 2 files changed, 83 insertions(+), 24 deletions(-)

diff --git a/libs/core/kiln_ai/utils/async_job_runner.py b/libs/core/kiln_ai/utils/async_job_runner.py
index 38203b2f..5650cfc5 100644
--- a/libs/core/kiln_ai/utils/async_job_runner.py
+++ b/libs/core/kiln_ai/utils/async_job_runner.py
@@ -1,15 +1,18 @@
 import asyncio
+import logging
 from dataclasses import dataclass
 from typing import AsyncGenerator, Awaitable, Callable, List, TypeVar
 
+logger = logging.getLogger(__name__)
+
 T = TypeVar("T")
 
 
 @dataclass
 class Progress:
-    complete: int | None = None
-    total: int | None = None
-    errors: int | None = None
+    complete: int
+    total: int
+    errors: int
 
 
 class AsyncJobRunner:
@@ -43,7 +46,7 @@ async def run(
         workers = []
         for _ in range(self.concurrency):
             task = asyncio.create_task(
-                self._run_worker(worker_queue, status_queue, run_job)
+                self._run_worker(worker_queue, status_queue, run_job),
             )
             workers.append(task)
 
@@ -80,9 +83,17 @@ async def _run_worker(
             except asyncio.QueueEmpty:
                 # worker can end when the queue is empty
                 break
+
             try:
                 success = await run_job(job)
+            except Exception as exc:
+                logger.exception("Job failed to complete", exc_info=exc)
+                success = False
+
+            try:
                 await status_queue.put(success)
+            except Exception as e:
+                logger.exception("Failed to enqueue status for job", exc_info=e)
             finally:
                 # Always mark the dequeued task as done, even on exceptions
                 worker_queue.task_done()
diff --git a/libs/core/kiln_ai/utils/test_async_job_runner.py b/libs/core/kiln_ai/utils/test_async_job_runner.py
index 83954a08..da596afa 100644
--- a/libs/core/kiln_ai/utils/test_async_job_runner.py
+++ b/libs/core/kiln_ai/utils/test_async_job_runner.py
@@ -1,8 +1,15 @@
+from typing import List
 from unittest.mock import AsyncMock
 
 import pytest
 
-from kiln_ai.utils.async_job_runner import AsyncJobRunner
+from kiln_ai.utils.async_job_runner import AsyncJobRunner, Progress
+
+
+@pytest.mark.parametrize("concurrency", [0, -1, -25])
+def test_invalid_concurrency_raises(concurrency):
+    with pytest.raises(ValueError):
+        AsyncJobRunner(concurrency=concurrency)
 
 
 # Test with and without concurrency
@@ -36,6 +43,32 @@ async def test_async_job_runner_status_updates(concurrency):
         mock_run_job_success.assert_any_await(jobs[i])
 
 
+# Test with and without concurrency
+@pytest.mark.parametrize("concurrency", [1, 25])
+@pytest.mark.asyncio
+async def test_async_job_runner_status_updates_empty_job_list(concurrency):
+    empty_job_list = []
+
+    runner = AsyncJobRunner(concurrency=concurrency)
+
+    # fake run_job that succeeds
+    mock_run_job_success = AsyncMock(return_value=True)
+
+    updates: List[Progress] = []
+    async for progress in runner.run(empty_job_list, mock_run_job_success):
+        updates.append(progress)
+
+    # Verify last status update was complete
+    assert len(updates) == 1
+
+    assert updates[0].complete == 0
+    assert updates[0].errors == 0
+    assert updates[0].total == 0
+
+    # Verify run_job was called for each job
+    assert mock_run_job_success.call_count == 0
+
+
 @pytest.mark.parametrize("concurrency", [1, 25])
 @pytest.mark.asyncio
 async def test_async_job_runner_all_failures(concurrency):
@@ -73,7 +106,7 @@ async def test_async_job_runner_partial_failures(concurrency):
     jobs = [{"id": i} for i in range(job_count)]
 
     # we want to fail on some jobs and succeed on others
-    jobs_to_fail = (0, 2, 4, 6, 8, 20, 25)
+    jobs_to_fail = set([0, 2, 4, 6, 8, 20, 25])
 
     runner = AsyncJobRunner(concurrency=concurrency)
 
@@ -88,7 +121,7 @@ async def test_async_job_runner_partial_failures(concurrency):
         assert progress.total == job_count
 
     # Verify last status update was complete
-    expected_error_count = len([job for job in jobs if job["id"] in jobs_to_fail])
+    expected_error_count = len(jobs_to_fail)
     expected_success_count = len(jobs) - expected_error_count
     assert progress.errors == expected_error_count
     assert progress.complete == expected_success_count
@@ -101,34 +134,49 @@ async def test_async_job_runner_partial_failures(concurrency):
         mock_run_job_partial_success.assert_any_await(jobs[i])
 
 
+@pytest.mark.parametrize("concurrency", [1, 25])
 @pytest.mark.asyncio
-async def test_async_job_runner_partial_raises():
+async def test_async_job_runner_partial_raises(concurrency):
     job_count = 50
     jobs = [{"id": i} for i in range(job_count)]
 
-    # we use concurrency=1 to avoid having the other workers complete jobs
-    # concurrently as that would make it hard to verify when the runner exits
-    runner = AsyncJobRunner(concurrency=1)
+    runner = AsyncJobRunner(concurrency=concurrency)
 
-    id_to_fail = 10
+    ids_to_fail = set([10, 25])
 
     def failure_fn(job):
-        if job["id"] == id_to_fail:
+        if job["id"] in ids_to_fail:
             raise Exception("job failed unexpectedly")
         return True
 
     # fake run_job that fails
     mock_run_job_partial_success = AsyncMock(side_effect=failure_fn)
 
+    # generate all the values we expect to see in progress updates
+    complete_values_expected = set([i for i in range(job_count - len(ids_to_fail) + 1)])
+    errors_values_expected = set([i for i in range(len(ids_to_fail) + 1)])
+
+    # keep track of all the updates we see
+    updates: List[Progress] = []
+
+    # we keep track of the progress values we have actually seen
+    complete_values_actual = set()
+    errors_values_actual = set()
+
     # Expect the status updates in order, and 1 for each job
-    # until we hit the job that raises an exception
-    with pytest.raises(Exception, match="job failed unexpectedly"):
-        expected_complete = 0
-        async for progress in runner.run(jobs, mock_run_job_partial_success):
-            assert progress.complete == expected_complete
-            assert progress.errors == 0
-            assert progress.total == job_count
-            expected_complete += 1
-
-    # verify that we yielded progress for jobs all the way up to the job that raised an exception
-    assert expected_complete == id_to_fail + 1
+    async for progress in runner.run(jobs, mock_run_job_partial_success):
+        updates.append(progress)
+        complete_values_actual.add(progress.complete)
+        errors_values_actual.add(progress.errors)
+
+        assert progress.total == job_count
+
+    # complete values should be all the jobs, except for the ones that failed
+    assert progress.complete == job_count - len(ids_to_fail)
+
+    # check that the actual updates and expected updates are equivalent sets
+    assert complete_values_actual == complete_values_expected
+    assert errors_values_actual == errors_values_expected
+
+    # we should have seen one update for each job, plus one for the initial status update
+    assert len(updates) == job_count + 1

From baff879179412e31d18e95aa435c8b77b25af934 Mon Sep 17 00:00:00 2001
From: "Leonard Q. Marcq" <marcqleonard@gmail.com>
Date: Wed, 14 May 2025 19:42:20 +0800
Subject: [PATCH 6/8] coderabbit feedback: better cleanup if exception

---
 libs/core/kiln_ai/utils/async_job_runner.py | 47 ++++++++++++---------
 1 file changed, 27 insertions(+), 20 deletions(-)

diff --git a/libs/core/kiln_ai/utils/async_job_runner.py b/libs/core/kiln_ai/utils/async_job_runner.py
index 5650cfc5..f0bba48d 100644
--- a/libs/core/kiln_ai/utils/async_job_runner.py
+++ b/libs/core/kiln_ai/utils/async_job_runner.py
@@ -50,26 +50,33 @@ async def run(
             )
             workers.append(task)
 
-        # Send status updates until workers are done, and they are all sent
-        while not status_queue.empty() or not all(worker.done() for worker in workers):
-            try:
-                # Use timeout to prevent hanging if all workers complete
-                # between our while condition check and get()
-                success = await asyncio.wait_for(status_queue.get(), timeout=0.1)
-                if success:
-                    complete += 1
-                else:
-                    errors += 1
-
-                yield Progress(complete=complete, total=total, errors=errors)
-            except asyncio.TimeoutError:
-                # Timeout is expected, just continue to recheck worker status
-                # Don't love this but beats sentinels for reliability
-                continue
-
-        # These are redundant, but keeping them will catch async errors
-        await asyncio.gather(*workers)
-        await worker_queue.join()
+        try:
+            # Send status updates until workers are done, and they are all sent
+            while not status_queue.empty() or not all(
+                worker.done() for worker in workers
+            ):
+                try:
+                    # Use timeout to prevent hanging if all workers complete
+                    # between our while condition check and get()
+                    success = await asyncio.wait_for(status_queue.get(), timeout=0.1)
+                    if success:
+                        complete += 1
+                    else:
+                        errors += 1
+
+                    yield Progress(complete=complete, total=total, errors=errors)
+                except asyncio.TimeoutError:
+                    # Timeout is expected, just continue to recheck worker status
+                    # Don't love this but beats sentinels for reliability
+                    continue
+        finally:
+            # Cancel outstanding workers on early exit or error
+            for w in workers:
+                w.cancel()
+
+            # These are redundant, but keeping them will catch async errors
+            await asyncio.gather(*workers)
+            await worker_queue.join()
 
     async def _run_worker(
         self,

From 02545dd85367072d4834ad20cac3aac8014697c4 Mon Sep 17 00:00:00 2001
From: "Leonard Q. Marcq" <marcqleonard@gmail.com>
Date: Wed, 14 May 2025 20:26:27 +0800
Subject: [PATCH 7/8] test: add test for higher-level exception

---
 .../kiln_ai/utils/test_async_job_runner.py    | 19 ++++++++++++++++++-
 1 file changed, 18 insertions(+), 1 deletion(-)

diff --git a/libs/core/kiln_ai/utils/test_async_job_runner.py b/libs/core/kiln_ai/utils/test_async_job_runner.py
index da596afa..be5d5c5e 100644
--- a/libs/core/kiln_ai/utils/test_async_job_runner.py
+++ b/libs/core/kiln_ai/utils/test_async_job_runner.py
@@ -1,5 +1,5 @@
 from typing import List
-from unittest.mock import AsyncMock
+from unittest.mock import AsyncMock, patch
 
 import pytest
 
@@ -180,3 +180,20 @@ def failure_fn(job):
 
     # we should have seen one update for each job, plus one for the initial status update
     assert len(updates) == job_count + 1
+
+
+@pytest.mark.parametrize("concurrency", [1, 25])
+@pytest.mark.asyncio
+async def test_async_job_runner_cancelled(concurrency):
+    runner = AsyncJobRunner(concurrency=concurrency)
+    jobs = [{"id": i} for i in range(10)]
+
+    with patch.object(
+        runner,
+        "_run_worker",
+        side_effect=Exception("run_worker raised an exception"),
+    ):
+        # if an exception is raised in the task, we should see it bubble up
+        with pytest.raises(Exception, match="run_worker raised an exception"):
+            async for _ in runner.run(jobs, AsyncMock(return_value=True)):
+                pass

From 6743226be213302615ab0e218d3b4c35484dbe16 Mon Sep 17 00:00:00 2001
From: "Leonard Q. Marcq" <marcqleonard@gmail.com>
Date: Wed, 14 May 2025 20:26:51 +0800
Subject: [PATCH 8/8] chore: log.error for consistency

---
 libs/core/kiln_ai/utils/async_job_runner.py | 8 ++++----
 1 file changed, 4 insertions(+), 4 deletions(-)

diff --git a/libs/core/kiln_ai/utils/async_job_runner.py b/libs/core/kiln_ai/utils/async_job_runner.py
index f0bba48d..fa4ba759 100644
--- a/libs/core/kiln_ai/utils/async_job_runner.py
+++ b/libs/core/kiln_ai/utils/async_job_runner.py
@@ -93,14 +93,14 @@ async def _run_worker(
 
             try:
                 success = await run_job(job)
-            except Exception as exc:
-                logger.exception("Job failed to complete", exc_info=exc)
+            except Exception:
+                logger.error("Job failed to complete", exc_info=True)
                 success = False
 
             try:
                 await status_queue.put(success)
-            except Exception as e:
-                logger.exception("Failed to enqueue status for job", exc_info=e)
+            except Exception:
+                logger.error("Failed to enqueue status for job", exc_info=True)
             finally:
                 # Always mark the dequeued task as done, even on exceptions
                 worker_queue.task_done()
